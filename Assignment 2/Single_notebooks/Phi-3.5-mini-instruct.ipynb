{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sexism Detection, Multi-class Classification, LLMs, Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFN9Q2rGLJnc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN4nmQacznGV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Relevant Material\n",
    "\n",
    "- Tutorial 3\n",
    "- Huggingface documentation\n",
    "- Huggingface hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck47wFhrLJnc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the [EDOS Task A](https://github.com/rewire-online/edos) on sexism detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mtjp0cGLJnc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Given an input text sentence, the task is to label the sentence as sexist or not sexist (binary classification).\n",
    "\n",
    "### Examples:\n",
    "\n",
    "**Text**: *``Schedule a date with her, then don't show up. Then text her \"GOTCHA B___H\".''*\n",
    "\n",
    "**Label**: Sexist\n",
    "\n",
    "**Text**: *``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.''*\n",
    "\n",
    "**Label**: Not sexist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dqwRZx-QNGX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approach\n",
    "\n",
    "We will tackle the binary classification task with LLMs.\n",
    "\n",
    "In particular, we'll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS3igwXpQcAY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We are going to download LLMs from [Huggingface](https://huggingface.co/).\n",
    "\n",
    "Many of these open-source LLMs require you to accept their \"Community License Agreement\" to download them.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- If not already, create an account of Huggingface (~2 mins)\n",
    "- Check a LLM model card page (e.g., [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)) and accept its \"Community License Agreement\".\n",
    "- Go to your account -> Settings -> Access Tokens -> Create new token -> \"Repositories permissions\" -> add the LLM model card you want to use.\n",
    "- Save the token (we'll need it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqEsPH_JSxw6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Huggingface Login\n",
    "\n",
    "Once we have created an account and an access token, we need to login to Huggingface via code.\n",
    "\n",
    "- Type your token and press Enter\n",
    "- You can say No to Github linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "_uWEUjs0THxP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLxSrY-4e_0J",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After login, you can download all models associated with your access token in addition to those that are not protected by an access token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEYMBnAQLJnc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "Since we are only interested in prompting, we do not require a train dataset.\n",
    "\n",
    "We have preparared a small test set version of EDOS in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material).\n",
    "\n",
    "Check the ``Assignment 2/data`` folder.\n",
    "It contains:\n",
    "\n",
    "- ``a2_test.csv`` → a small test set of 300 samples.\n",
    "- ``demonstrations.csv`` -> a batch of 1000 samples for few-shot prompting.\n",
    "\n",
    "Both datasets contain a balanced number of sexist and not sexist samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5XyOcFGLJnd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "We require you to:\n",
    "\n",
    "* **Download** the ``A2/data`` folder.\n",
    "* **Encode** ``a2_test.csv`` into a ``pandas.DataFrame`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_sexist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-17133</td>\n",
       "      <td>The boys must be gaming because there goes the...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-14197</td>\n",
       "      <td>Look at those eyes. Either someone unexpectedl...</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-3018</td>\n",
       "      <td>Old man mogs everyone in this sub</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-5301</td>\n",
       "      <td>Excellent, I was just looking at another post ...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-17796</td>\n",
       "      <td>So you run back to daddy whenever you need hel...</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rewire_id  \\\n",
       "0  sexism2022_english-17133   \n",
       "1  sexism2022_english-14197   \n",
       "2   sexism2022_english-3018   \n",
       "3   sexism2022_english-5301   \n",
       "4  sexism2022_english-17796   \n",
       "\n",
       "                                                text label_sexist  \n",
       "0  The boys must be gaming because there goes the...   not sexist  \n",
       "1  Look at those eyes. Either someone unexpectedl...       sexist  \n",
       "2                  Old man mogs everyone in this sub   not sexist  \n",
       "3  Excellent, I was just looking at another post ...   not sexist  \n",
       "4  So you run back to daddy whenever you need hel...       sexist  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode a2_test.csv into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(PATH + 'a2_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJp08l4yLJnd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Model setup\n",
    "\n",
    "Once the test data has been loaded, we have to setup the model pipeline for inference.\n",
    "\n",
    "In particular, we have to:\n",
    "- Load the model weights from Huggingface\n",
    "- Quantize the model to fit into a single-GPU limited hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptffotFIjq89",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which LLMs?\n",
    "\n",
    "The pool of LLMs is ever increasing and it's impossible to keep track of all new entries.\n",
    "\n",
    "We focus on popular open-source models.\n",
    "\n",
    "- [Mistral v2](mistralai/Mistral-7B-Instruct-v0.2)\n",
    "- [Mistral v3](mistralai/Mistral-7B-Instruct-v0.3)\n",
    "- [Llama v3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "- [Phi3-mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "\n",
    "Other open-source models are more than welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH1YShLfLJnd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 1 points, we require you to:\n",
    "\n",
    "* Pick 2 model cards from the provided list.\n",
    "* For each model:\n",
    "  - Define a separate section of your notebook for the model.\n",
    "  - Setup a quantization configuration for the model.\n",
    "  - Load the model via HuggingFace APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8ORld7klCfG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "1. There's a popular library integrated with Huggingface's ``transformers`` to perform quantization.\n",
    "\n",
    "2. Define two separate sections of your notebook to show that you have implemented the prompting pipeline for each selected model card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3.5 mini instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "\n",
    "model_card = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0bb44cd44543ae8f2cf3f35b7bf842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_card,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 2\n",
    "#generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "#generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.do_sample = False\n",
    "generation_config.temperature = None\n",
    "generation_config.top_p = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Total GPU memory: 8.00 GB\n",
      "Allocated GPU memory: 4.23 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "print(f\"Total GPU memory: {total_memory / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzNNzb1VLJnd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 1.0 points] Prompt setup\n",
    "\n",
    "Prompting requires an input pre-processing phase where we convert each input example into a specific instruction prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GaBtKXomY_m",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prompt Template\n",
    "\n",
    "Use the following prompt template to process input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "7e8P-Kk8me6q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sexism detection.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
    "\n",
    "        TEXT:\n",
    "        {text}\n",
    "\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Hugging Face custom handling function on the prompt to create our final template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an annotator for sexism detection.<|end|>\n",
      "<|user|>\n",
      "Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
      "\n",
      "        TEXT:\n",
      "        {text}\n",
      "\n",
      "        ANSWER:\n",
      "        <|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the prompt on a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an annotator for sexism detection. Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
      "\n",
      "        TEXT:\n",
      "        Ahahahah I fucking hate women. They are so stupid.\n",
      "\n",
      "        ANSWER:\n",
      "         YES\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Ahahahah I fucking hate women. They are so stupid.\"\n",
    "formatted_example = prompt.format(text=example_text)\n",
    "parsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHeoEN7MLJnd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 2 points, we require you to:\n",
    "\n",
    "* Write a ``prepare_prompts`` function as the one reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "PUjRVMtMm9CE",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_prompts(texts, prompt_template, tokenizer):\n",
    "    \"\"\"\n",
    "      This function format input text samples into instructions prompts.\n",
    "\n",
    "      Inputs:\n",
    "        texts: input texts to classify via prompting\n",
    "        prompt_template: the prompt template provided in this assignment\n",
    "        tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
    "\n",
    "      Outputs:\n",
    "        input texts to classify in the form of instruction prompts\n",
    "    \"\"\"\n",
    "    tokenized_prompts = []\n",
    "    texts_len = len(texts)\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        full_prompt = prompt_template.format(text=text)\n",
    "        tokenized_prompt = tokenizer(full_prompt, return_tensors='pt').to('cuda')\n",
    "        tokenized_prompts.append(tokenized_prompt)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Tokenized {i}/{texts_len} prompts\")\n",
    "\n",
    "    return tokenized_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0/2 prompts\n",
      "[{'input_ids': tensor([[32006,   887,   526,   385,  9732,  1061,   363,  7916,  1608, 15326,\n",
      "         29889, 32007, 32010,  3575,  3414,   338,   304,   770,  1598,  1881,\n",
      "          1426,   408,  6943,  7916,  1608,   470,   451, 29889,  2538,  2818,\n",
      "           871, 22483,   470, 11698, 29889,    13,    13,  4706,   323, 12194,\n",
      "         29901,    13,  4706,   379,  2350, 25613,   306,   285,  2707,   292,\n",
      "         26277,  5866, 29889,    13,    13,  4706,   319,  3059, 29956,  1001,\n",
      "         29901,    13,   308, 32007, 32001]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}, {'input_ids': tensor([[32006,   887,   526,   385,  9732,  1061,   363,  7916,  1608, 15326,\n",
      "         29889, 32007, 32010,  3575,  3414,   338,   304,   770,  1598,  1881,\n",
      "          1426,   408,  6943,  7916,  1608,   470,   451, 29889,  2538,  2818,\n",
      "           871, 22483,   470, 11698, 29889,    13,    13,  4706,   323, 12194,\n",
      "         29901,    13,  4706, 15043, 29892,   590,  1024,   338,  2259, 29889,\n",
      "            13,    13,  4706,   319,  3059, 29956,  1001, 29901,    13,   308,\n",
      "         32007, 32001]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Hhaaha I fucking hate women.\", \"Hello, my name is John.\"]\n",
    "tokenized_prompts = prepare_prompts(texts, prompt, tokenizer)\n",
    "print(tokenized_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOCRgGf7mifk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "1. You are free to modify the prompt format (**not its content**) as you like depending on your code implementation.\n",
    "\n",
    "2. Note that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgBhkBwuLJnd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Inference\n",
    "\n",
    "We are now ready to define the inference loop where we prompt the model with each pre-processed sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WsrQSvcLJnd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 3 points, we require you to:\n",
    "\n",
    "* Write a ``generate_responses`` function as the one reported below.\n",
    "* Write a ``process_response`` function as the one reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "bG3CDXNlyD5k",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, prompt_examples):\n",
    "  \"\"\"\n",
    "    This function implements the inference loop for a LLM model.\n",
    "    Given a set of examples, the model is tasked to generate a response.\n",
    "\n",
    "    Inputs:\n",
    "      model: LLM model instance for prompting\n",
    "      prompt_examples: pre-processed text samples\n",
    "\n",
    "    Outputs:\n",
    "      generated responses\n",
    "  \"\"\"\n",
    "  responses = []\n",
    "\n",
    "  for prompt in prompt_examples:\n",
    "    outputs = model.generate(**prompt, max_new_tokens=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    responses.append(response)\n",
    "\n",
    "  return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of tokens for the output has to be slightly higher than 2, since the model needs some spacing margin to generate the answer.  \n",
    "Since we have only two possible answers, we set the `max_new_tokens` value to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an annotator for sexism detection. Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
      "\n",
      "        TEXT:\n",
      "        Hhaaha I fucking hate women.\n",
      "\n",
      "        ANSWER:\n",
      "         YES\n"
     ]
    }
   ],
   "source": [
    "responses = generate_responses(model, tokenized_prompts)\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "uiCoMrutyXTU",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    \"\"\"\n",
    "      This function takes a textual response generated by the LLM\n",
    "      and processes it to map the response to a binary label.\n",
    "\n",
    "      Inputs:\n",
    "        response: generated response from LLM\n",
    "\n",
    "      Outputs:\n",
    "        parsed binary response: return 1 if YES and 0 if NO\n",
    "    \"\"\"\n",
    "    result = response.split(\"\\n\")[-1].strip().lower()\n",
    "    if \"yes\" in result:\n",
    "        return 1\n",
    "    elif \"no\" in result:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 # Invalid response, it will then casted to 0 for accuracy calculation. It is used for the fail rate calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_response(responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the inference loop on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(df, model, prompt, tokenizer):\n",
    "    \"\"\"\n",
    "    This function runs inference on a dataset using the LLM model.\n",
    "\n",
    "    Inputs:\n",
    "        df: input dataset\n",
    "        model: LLM model instance for prompting\n",
    "        prompt: the instruction prompt template\n",
    "        tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
    "\n",
    "    Outputs:\n",
    "        predictions: binary predictions for the input dataset\n",
    "    \"\"\"\n",
    "    texts = df['text']\n",
    "    tokenized_prompts = prepare_prompts(texts, prompt, tokenizer)\n",
    "    print(\"Tokenized prompts generated.\")\n",
    "    responses = generate_responses(model, tokenized_prompts)\n",
    "    print(\"Responses generated.\")\n",
    "    predictions = [process_response(response) for response in responses]\n",
    "    print(\"Predictions generated.\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0/300 prompts\n",
      "Tokenized 50/300 prompts\n",
      "Tokenized 100/300 prompts\n",
      "Tokenized 150/300 prompts\n",
      "Tokenized 200/300 prompts\n",
      "Tokenized 250/300 prompts\n",
      "Tokenized prompts generated.\n",
      "Responses generated.\n",
      "Predictions generated.\n"
     ]
    }
   ],
   "source": [
    "#Run inference on the dataset\n",
    "predictions = run_inference(df, model, prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKcQ0ZfO2Fya",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "1. According to our tests, it should take you ~10 mins to perform full inference on 300 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyZ8WU09zz-a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 0.5 points] Metrics\n",
    "\n",
    "In order to evaluate selected LLMs, we need to compute performance metrics.\n",
    "\n",
    "In particular, we are interested in computing **accuracy** since the provided data is balanced with respect to classification classes.\n",
    "\n",
    "Moreover, we want to compute the ratio of failed responses generated by models. \n",
    "\n",
    "That is, how frequent the LLM fails to follow instructions and provides incorrect responses that do not address the classification task.\n",
    "\n",
    "We denote this metric as **fail-ratio**.\n",
    "\n",
    "In summary, we parse generated responses as follows:\n",
    "- 1 if the model says YES\n",
    "- 0 if the model says NO\n",
    "- 0 if the model does not answer in either way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6lu64o80iX4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 4 points, we require you to:\n",
    "\n",
    "* Write a ``compute_metrics`` function as the one reported below.\n",
    "* Compute metrics for the two selected LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9Fmcw_9v0k9D",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(responses, y_true):\n",
    "    \"\"\"\n",
    "      This function takes predicted and ground-truth labels and compute metrics.\n",
    "      In particular, this function compute accuracy and fail-ratio metrics.\n",
    "      This function internally invokes `process_response` to compute metrics.\n",
    "\n",
    "      Inputs:\n",
    "        responses: generated LLM responses\n",
    "        y_true: ground-truth binary labels\n",
    "\n",
    "      Outputs:\n",
    "        dictionary containing desired metrics\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    fail_ratio = 0\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        if response == y_true[i]:\n",
    "            accuracy += 1\n",
    "        elif response == -1 and y_true[i] == 0:\n",
    "            accuracy += 1\n",
    "        if response == -1:\n",
    "            fail_ratio += 1\n",
    "    \n",
    "    accuracy /= len(y_true)\n",
    "    fail_ratio /= len(y_true)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": round(accuracy, 4),\n",
    "        \"fail_ratio\": round(fail_ratio, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.59, 'fail_ratio': 0.0}\n"
     ]
    }
   ],
   "source": [
    "y_true = df['label_sexist'].apply(lambda x: 1 if x == 'sexist' else 0).tolist()\n",
    "metrics = compute_metrics(df['predictions'], y_true)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHyvV4QD2vZS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Few-shot Inference\n",
    "\n",
    "So far, we have tested models in a zero-shot fashion: we provide the input text to classify and instruct the model to generate a response.\n",
    "\n",
    "We are now interested in performing few-shot prompting to see the impact of providing demonstration examples.\n",
    "\n",
    "To do so, we slightly change the prompt template as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "pEqsOHz63ReW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sexism detection.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
    "\n",
    "        EXAMPLES:\n",
    "        {examples}\n",
    "\n",
    "        TEXT:\n",
    "        {text}\n",
    "\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE0XnaVr3CZz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "In order to get Task 5 points, we require you to:\n",
    "\n",
    "- Load ``demonstrations.csv`` and encode it into a ``pandas.DataFrame`` object.\n",
    "- Define a ``build_few_shot_demonstrations`` function as the one reported below.\n",
    "- Perform few-shot inference as in Task 3.\n",
    "- Compute metrics as in Task 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load ``demonstrations.csv`` and encode it into a ``pandas.DataFrame`` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_sexist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-10348</td>\n",
       "      <td>I would never have my kid go to a daycare with...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-7950</td>\n",
       "      <td>Democrat women are so butt fucking ugly.</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-7742</td>\n",
       "      <td>If you really wanted to twist the knife you’d ...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-16170</td>\n",
       "      <td>Those women are pro-rape. They didn’t care abo...</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-9857</td>\n",
       "      <td>Married a bitch... Is it the standard scenario...</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rewire_id  \\\n",
       "0  sexism2022_english-10348   \n",
       "1   sexism2022_english-7950   \n",
       "2   sexism2022_english-7742   \n",
       "3  sexism2022_english-16170   \n",
       "4   sexism2022_english-9857   \n",
       "\n",
       "                                                text label_sexist  \n",
       "0  I would never have my kid go to a daycare with...   not sexist  \n",
       "1           Democrat women are so butt fucking ugly.       sexist  \n",
       "2  If you really wanted to twist the knife you’d ...   not sexist  \n",
       "3  Those women are pro-rape. They didn’t care abo...       sexist  \n",
       "4  Married a bitch... Is it the standard scenario...       sexist  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demonstrations = pd.read_csv(PATH + 'demonstrations.csv')\n",
    "demonstrations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a ``build_few_shot_demonstrations`` function as the one reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "6oyxypUG3ltH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_few_shot_demonstrations(demonstrations, num_per_class=2):\n",
    "  \"\"\"\n",
    "    Inputs:\n",
    "      demonstrations: the pandas.DataFrame object wrapping demonstrations.csv\n",
    "      num_per_class: number of demonstrations per class\n",
    " \n",
    "    Outputs:\n",
    "      a list of textual demonstrations to inject into the prompt template.\n",
    "  \"\"\"\n",
    "  \n",
    "  examples = []\n",
    "  sexist_demonstrations = demonstrations[demonstrations['label_sexist'] == 'sexist']['text'].sample(num_per_class)\n",
    "  non_sexist_demonstrations = demonstrations[demonstrations['label_sexist'] == 'not sexist']['text'].sample(num_per_class)\n",
    "  examples.extend(sexist_demonstrations)\n",
    "  examples.extend(non_sexist_demonstrations)\n",
    "\n",
    "  for i in range(len(examples)):\n",
    "    examples[i] =  f\"TEXT: {examples[i]}\\nANSWER: YES\" if i < num_per_class else f\"TEXT: {examples[i]}\\nANSWER: NO\"\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: nothing irks a left wing feminist worse than seeing a beautiful virtuous woman who is conservative they HATE her H-A-T-E so they try to suppress her next it will be illegal to be born beautiful they'll call it \"Hate Birth\"\n",
      "ANSWER: YES\n",
      "TEXT: Get #TRIGGERED, Hillaryfags, femme-fags, and feminazis: [URL] #MAGA #TRUMP #ELECTION2016 [URL]\n",
      "ANSWER: YES\n",
      "TEXT: Reposting this since all the mailboxes were full over the weekend. Please everybody call! #ReleaseTheMemo\n",
      "ANSWER: NO\n",
      "TEXT: she is mental......they are teaching whites to hate their own children....\n",
      "ANSWER: NO\n"
     ]
    }
   ],
   "source": [
    "num_per_class = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "examples = build_few_shot_demonstrations(demonstrations, num_per_class)\n",
    "formatted_examples = \"\\n\".join(examples)\n",
    "print(formatted_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform few-shot inference as in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_few_shots(texts, examples, prompt_template, tokenizer):\n",
    "    \"\"\"\n",
    "      This function format input text samples into instructions prompts.\n",
    "\n",
    "      Inputs:\n",
    "        texts: input texts to classify via prompting\n",
    "        examples: list of examples to inject into the prompt template\n",
    "        prompt_template: the prompt template provided in this assignment\n",
    "        tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
    "\n",
    "      Outputs:\n",
    "        input texts to classify in the form of instruction prompts\n",
    "    \"\"\"\n",
    "    tokenized_prompts = []\n",
    "    texts_len = len(texts)\n",
    "    formatted_examples = \"\\n\".join(examples)\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        full_prompt = prompt_template.format(text=text, examples=formatted_examples)\n",
    "        tokenized_prompt = tokenizer(full_prompt, return_tensors='pt').to('cuda')\n",
    "        tokenized_prompts.append(tokenized_prompt)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Tokenized {i}/{texts_len} prompts\")\n",
    "\n",
    "    return tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_few_shots(df, model, prompt, tokenizer, num_per_class):\n",
    "    \"\"\"\n",
    "    This function runs inference on a dataset using the LLM model.\n",
    "\n",
    "    Inputs:\n",
    "        df: input dataset\n",
    "        model: LLM model instance for prompting\n",
    "        prompt: the instruction prompt template\n",
    "        tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n",
    "        num_per_class: number of examples for each class to inject into the prompt template\n",
    "\n",
    "    Outputs:\n",
    "        predictions: binary predictions for the input dataset\n",
    "    \"\"\"\n",
    "    texts = df['text']\n",
    "    examples = build_few_shot_demonstrations(demonstrations, num_per_class)\n",
    "    tokenized_prompts = prepare_prompts_few_shots(texts, examples, prompt, tokenizer)\n",
    "    print(\"Tokenized prompts generated.\")\n",
    "    responses = generate_responses(model, tokenized_prompts)\n",
    "    print(\"Responses generated.\")\n",
    "    predictions = [process_response(response) for response in responses]\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == -1:\n",
    "            print(responses[i])\n",
    "    print(\"Predictions generated.\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the inference on the dataset, starting with 2 examples per classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0/300 prompts\n",
      "Tokenized 50/300 prompts\n",
      "Tokenized 100/300 prompts\n",
      "Tokenized 150/300 prompts\n",
      "Tokenized 200/300 prompts\n",
      "Tokenized 250/300 prompts\n",
      "Tokenized prompts generated.\n",
      "Responses generated.\n",
      "Predictions generated.\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "predictions = run_inference_few_shots(df, model, prompt, tokenizer, num_per_class)\n",
    "df['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6467, 'fail_ratio': 0.0}\n",
      "1    196\n",
      "0    104\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_true = df['label_sexist'].apply(lambda x: 1 if x == 'sexist' else 0).tolist()\n",
    "metrics = compute_metrics(df['predictions'], y_true)\n",
    "print(metrics)\n",
    "print(df.value_counts(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0/300 prompts\n",
      "Tokenized 50/300 prompts\n",
      "Tokenized 100/300 prompts\n",
      "Tokenized 150/300 prompts\n",
      "Tokenized 200/300 prompts\n",
      "Tokenized 250/300 prompts\n",
      "Tokenized prompts generated.\n",
      "Responses generated.\n",
      "Predictions generated.\n",
      "{'accuracy': 0.6767, 'fail_ratio': 0.0}\n",
      "0    167\n",
      "1    133\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_per_class = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "examples = build_few_shot_demonstrations(demonstrations, num_per_class)\n",
    "\n",
    "predictions = run_inference_few_shots(df, model, prompt, tokenizer, num_per_class)\n",
    "df['predictions'] = predictions\n",
    "\n",
    "y_true = df['label_sexist'].apply(lambda x: 1 if x == 'sexist' else 0).tolist()\n",
    "metrics = compute_metrics(df['predictions'], y_true)\n",
    "print(metrics)\n",
    "print(df.value_counts(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGpZWQb45XXK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "1. You are free to pick any value for ``num_per_class``.\n",
    "\n",
    "2. According to our tests, few-shot prompting increases inference time by some minutes (we experimented with ``num_per_class`` $\\in [2, 4]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHuT1a1GLJnd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "We are now interested in evaluating model responses and comparing their performance.\n",
    "\n",
    "This analysis helps us in understanding\n",
    "\n",
    "- Classification task performance gap: are the models good at this task?\n",
    "- Generation quality: which kind of responses do models generate?\n",
    "- Errors: which kind of mistakes do models do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kjEAHD4LJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 6 points, we require you to:\n",
    "\n",
    "* Compare classification performance of selected LLMs in a Table.\n",
    "* Compute confusion matrices for selected LLMs.\n",
    "* Briefly summarize your observations on generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QWlVXJgLJne",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fsdV99TLJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-hUXYaLLJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fURV8zfPLJne",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn1tUeYzLJne",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYAOVGvKhtTQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model cards\n",
    "\n",
    "You can pick any open-source model card you like.\n",
    "\n",
    "We recommend starting from those reported in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWG72N-LLJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Everything can be done via ``transformers`` APIs.\n",
    "\n",
    "However, you are free to test frameworks, such as [LangChain](https://www.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/) [LitParrot](https://github.com/awesome-software/lit-parrot), provided that you correctly address task instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cplnq3dLJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bonus Points\n",
    "\n",
    "0.5 bonus points are arbitrarily assigned based on significant contributions such as:\n",
    "\n",
    "- Outstanding error analysis\n",
    "- Masterclass code organization\n",
    "- Suitable extensions\n",
    "- Evaluate A1 dataset and perform comparison\n",
    "\n",
    "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coy_pJ40LJne",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prompt Template\n",
    "\n",
    "Do not change the provided prompt template.\n",
    "\n",
    "You are only allowed to change it in case of a possible extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSiH0Xqj79wc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimizations\n",
    "\n",
    "Any kind of code optimization (e.g., speedup model inference or reduce computational cost) is more than welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpr-LSK7LJnh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
